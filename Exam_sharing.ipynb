{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Untitled11.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "hJ_CV0U9m-X-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import pandas as pd\n",
        "train = pd.read_csv('jigsaw-toxic-comment-train.csv')\n",
        "val = pd.read_csv('validation.csv')\n",
        "pd.set_option('display.max_columns', 100)\n",
        "pd.set_option('display.max_rows', 100)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fUgGAQCKnEGV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import re \n",
        "from string import punctuation\n",
        "\n",
        "# делаем минимум нормализации т.к. хотим учитывать как можно больше признаков\n",
        "def soft_normalize(text):\n",
        "  text = re.sub(\"\\n|\\t|\\s\\\"\", \" \", text)\n",
        "  return text\n",
        "\n",
        "def tokenize(text):\n",
        "  text = re.sub(f\"[{punctuation}]\", \" \", text)\n",
        "  text = re.sub(\"\\s+\", \" \", text)\n",
        "  return text.split()\n",
        "\n",
        "def sentenize(text):\n",
        "  sents = re.split(\"[\\n.!?]( |$)\", text)\n",
        "  return [s for s in sents if len(s) > 0 and s != \" \"]\n",
        "\n",
        "def sents_count(text):\n",
        "  return len(sentenize(text))\n",
        "\n",
        "def mean_token_length(text):\n",
        "  length = 0\n",
        "  tokens = tokenize(text)\n",
        "  if len(tokens) == 0:\n",
        "    return 0\n",
        "  for token in tokens:\n",
        "    length += len(token)\n",
        "  return length/len(tokens)\n",
        "\n",
        "def caps_ratio(text):\n",
        "  total = len(text)\n",
        "  caps = 0\n",
        "  for letter in text:\n",
        "    if letter not in punctuation and letter != \" \":\n",
        "      if letter.lower() != letter:\n",
        "        caps += 1\n",
        "  return caps/total\n",
        "\n",
        "def nonalpha_ratio(text):\n",
        "  total = len(text)\n",
        "  nonalpha = 0\n",
        "  sents = sentenize(text)\n",
        "  for sent in sents:\n",
        "    for letter in text:\n",
        "      if letter in punctuation:\n",
        "        nonalpha += 1\n",
        "  return nonalpha/total\n",
        "\n",
        "def numeric_ratio(text):\n",
        "  total = len(text)\n",
        "  numeric = 0\n",
        "  for letter in text:\n",
        "    if letter.isnumeric():\n",
        "      numeric += 1\n",
        "  return numeric/total\n",
        "\n",
        "def has_link(text):\n",
        "  return int(re.search(\"https?://[^ ]+\", text) is not None)\n",
        "\n",
        "def longest_word(text):\n",
        "  tokens = tokenize(text)\n",
        "  length = 0\n",
        "  for t in tokens:\n",
        "    if len(t) > length:\n",
        "      length = len(t)\n",
        "  return length\n",
        "\n",
        "def max_sent_complexity(text):\n",
        "  compl = 0\n",
        "  sents = sentenize(text)\n",
        "  for s in sents:\n",
        "    matches = re.findall(\"[,;\\:\\-()]+\", s)\n",
        "    if len(matches) > compl:\n",
        "      compl = len(matches)\n",
        "  return compl\n",
        "\n",
        "def rage_punctuation_length(text):\n",
        "  length = 0\n",
        "  matches = re.findall(\"[?!.]+\", text)\n",
        "  for m in matches:\n",
        "    if len(m) > length:\n",
        "      length = len(m)\n",
        "  return length\n",
        "\n",
        "def longest_same_char(text):\n",
        "  length = 0\n",
        "  curr_length = 1\n",
        "  for i in range(1,len(text)):\n",
        "    if text[i] == text[i-1]:\n",
        "      curr_length += 1\n",
        "    else:\n",
        "      if curr_length > length:\n",
        "        length = curr_length\n",
        "      curr_length = 1\n",
        "  if curr_length > length:\n",
        "    return curr_length\n",
        "  return length "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "q_4UHaHwnFOW",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train['text_soft_normalized'] = train['comment_text'].apply(soft_normalize)\n",
        "\n",
        "train['text_mean_tokens_length'] = train['text_soft_normalized'].apply(mean_token_length)\n",
        "train['text_sents_count'] = train['comment_text'].apply(sents_count)\n",
        "train['text_caps_ratio'] = train['comment_text'].apply(caps_ratio)\n",
        "train['text_nonalpha_ratio'] = train['text_soft_normalized'].apply(nonalpha_ratio)\n",
        "train['text_numeric_ratio'] = train['comment_text'].apply(numeric_ratio)\n",
        "train['text_has_link'] = train['comment_text'].apply(has_link)\n",
        "train['text_longest_word'] = train['text_soft_normalized'].apply(longest_word)\n",
        "train['text_complexity'] = train['text_soft_normalized'].apply(max_sent_complexity)\n",
        "train['text_rage_punctuation_length'] = train['text_soft_normalized'].apply(rage_punctuation_length)\n",
        "train['text_longest_same_char'] = train['text_soft_normalized'].apply(longest_same_char)\n",
        "\n",
        "val['text_soft_normalized'] = val['comment_text'].apply(soft_normalize)\n",
        "\n",
        "val['text_mean_tokens_length'] = val['text_soft_normalized'].apply(mean_token_length)\n",
        "val['text_sents_count'] = val['comment_text'].apply(sents_count)\n",
        "val['text_caps_ratio'] = val['comment_text'].apply(caps_ratio)\n",
        "val['text_nonalpha_ratio'] = val['text_soft_normalized'].apply(nonalpha_ratio)\n",
        "val['text_numeric_ratio'] = val['comment_text'].apply(numeric_ratio)\n",
        "val['text_has_link'] = val['comment_text'].apply(has_link)\n",
        "val['text_longest_word'] = val['text_soft_normalized'].apply(longest_word)\n",
        "val['text_complexity'] = val['text_soft_normalized'].apply(max_sent_complexity)\n",
        "val['text_rage_punctuation_length'] = val['text_soft_normalized'].apply(rage_punctuation_length)\n",
        "val['text_longest_same_char'] = val['text_soft_normalized'].apply(longest_same_char)"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}